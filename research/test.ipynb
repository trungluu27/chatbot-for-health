{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HIHI\n"
     ]
    }
   ],
   "source": [
    "print('HIHI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract Data From the PDF File\n",
    "def load_pdf_file(data):\n",
    "    loader= DirectoryLoader(data,\n",
    "                            glob=\"*.pdf\",\n",
    "                            loader_cls=PyPDFLoader)\n",
    "\n",
    "    documents=loader.load()\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracted_data\n",
    "extracted_data=load_pdf_file(data='data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the Data into Text Chunks\n",
    "def text_split(extracted_data):\n",
    "    text_splitter=RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=50)\n",
    "    text_chunks=text_splitter.split_documents(extracted_data)\n",
    "    return text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Text Chunks 135\n"
     ]
    }
   ],
   "source": [
    "# text_chunks\n",
    "text_chunks=text_split(extracted_data)\n",
    "print(\"Length of Text Chunks\", len(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download the Embeddings from Hugging Face\n",
    "def download_hugging_face_embeddings():\n",
    "    embeddings=HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LUU THE TRUNG\\AppData\\Local\\Temp\\ipykernel_18156\\2661704553.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings=HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
      "c:\\Users\\LUU THE TRUNG\\anaconda3\\envs\\healthybot_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "embeddings = download_hugging_face_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length 384\n"
     ]
    }
   ],
   "source": [
    "query_result = embeddings.embed_query(\"Xin chào\")\n",
    "print(\"Length\", len(query_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use vector db Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# PINECONE_API_KEY=os.environ.get('PINECONE_API_KEY')\n",
    "# OPENAI_API_KEY=os.environ.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "# from pinecone import ServerlessSpec\n",
    "\n",
    "\n",
    "# pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "# index_name = \"healthbot\"\n",
    "\n",
    "\n",
    "# pc.create_index(\n",
    "#     name=index_name,\n",
    "#     dimension=384, \n",
    "#     metric=\"cosine\", \n",
    "#     spec=ServerlessSpec(\n",
    "#         cloud=\"aws\", \n",
    "#         region=\"us-east-1\"\n",
    "#     ) \n",
    "# ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed each chunk and upsert the embeddings into your Pinecone index.\n",
    "# from langchain.vectorstores import Pinecone\n",
    "\n",
    "# docsearch = Pinecone.from_documents(\n",
    "#     documents=text_chunks,\n",
    "#     index_name=index_name,\n",
    "#     embedding=embeddings, \n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Existing index \n",
    "\n",
    "# from langchain.vectorstores import Pinecone\n",
    "# # Embed each chunk and upsert the embeddings into your Pinecone index.\n",
    "# docsearch = Pinecone.from_existing_index(\n",
    "#     index_name=index_name,\n",
    "#     embedding=embeddings\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieved_docs = retriever.invoke(\"Bé bị sốt, phải làm sao?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieved_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use vector DB Faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x2795ac237f0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "vector_db_path = 'vector_db/db_faiss'\n",
    "\n",
    "def create_db():\n",
    "    db = FAISS.from_documents(text_chunks, embeddings)\n",
    "    db.save_local(vector_db_path)\n",
    "    return db\n",
    "\n",
    "create_db()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_openai import OpenAI\n",
    "# llm = OpenAI(temperature=0.4, max_tokens=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.chains import create_retrieval_chain\n",
    "# from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "# system_prompt = (\n",
    "#     \"Bạn là trợ lý ảo có nhiệm vụ trả lời các câu hỏi về sức khỏe. \"\n",
    "#     \"Sử dụng các thông tin trong tài liệu để trả lời câu hỏi. \"\n",
    "#     \"Nếu không biết câu trả lời, hãy nói rằng bạn không biết. \"\n",
    "#     \"Sử dụng ngôn ngữ tự nhiên và trả lời ngắn gọn.\"\n",
    "#     \"\\n\\n\"\n",
    "#     \"{context}\"\n",
    "# )\n",
    "\n",
    "\n",
    "# prompt = ChatPromptTemplate.from_messages(\n",
    "#     [\n",
    "#         (\"system\", system_prompt),\n",
    "#         (\"human\", \"{input}\"),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "# rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "# rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = rag_chain.invoke({\"input\": \"Bé bị sốt, phải làm sao?\"})\n",
    "# print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use HuggingFace "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import CTransformers\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = 'models/vinallama-7b-chat_q5_0.gguf'\n",
    "\n",
    "def load_model(model_file):\n",
    "    llm = CTransformers(\n",
    "        model=model_file,\n",
    "        model_type=\"llama\",\n",
    "        max_new_tokens=1024,\n",
    "        temperature=0.01\n",
    "    )\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def creart_prompt(template):\n",
    "#     prompt = PromptTemplate(template=template, input_variables=['question'])\n",
    "#     return prompt\n",
    "\n",
    "# def create_simple_chain(prompt, llm):\n",
    "#     chain = LLMChain(prompt=prompt, llm=llm)\n",
    "#     return chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# template = '''\n",
    "# <|im_start|>system\n",
    "# Bạn là trợ lý ảo hữu ích. Hãy trả lời người dùng chính xác. Sử dụng ngôn ngữ tự nhiên và trả lời ngắn gọn.\n",
    "# <|im_end|>\n",
    "# <|im_start|>user\n",
    "# {question}<|im_end|>\n",
    "# <|im_start|>assistant\n",
    "# '''\n",
    "\n",
    "# prompt = creart_prompt(template)\n",
    "# llm = load_model(model_file)\n",
    "# chain = create_simple_chain(prompt, llm)\n",
    "\n",
    "# response = chain.invoke({\"question\": \"Ai là tổng thống Mỹ năm 2024?\"})\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creart_prompt(template):\n",
    "    prompt = PromptTemplate(template=template, input_variables=['context', 'question'])\n",
    "    return prompt\n",
    "\n",
    "def create_qa_chain(prompt, llm, db):\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=db.as_retriever(search_kwargs={\"k\":2}),\n",
    "        chain_type_kwargs={\"prompt\": prompt}\n",
    "    )\n",
    "    return qa_chain\n",
    "\n",
    "def read_vector_db():\n",
    "    db = FAISS.load_local(vector_db_path, embeddings, allow_dangerous_deserialization=True)\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = '''\n",
    "<|im_start|>system\n",
    "Bạn là trợ lý ảo có nhiệm vụ trả lời các câu hỏi về sức khỏe. Sử dụng các thông tin trong tài liệu để trả lời câu hỏi. Nếu không biết câu trả lời, hãy nói rằng bạn không biết. Sử dụng ngôn ngữ tự nhiên và trả lời ngắn gọn.\n",
    "{context}<|im_end|>\n",
    "<|im_start|>user\n",
    "{question}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'Bé bị sốt, phải làm sao?', 'result': 'Nếu trẻ dưới 6 tháng tuổi và bị sốt, bạn nên gọi trợ giúp y tế nếu trẻ:\\n- Bị cứng cổ.\\n- Có các vấn đề khác ngoài sốt như nôn mửa, cứng tai, phát ban hoặc co giật.\\n\\nĐối với trẻ trên 6 tháng tuổi và bị sốt kéo dài hơn 48 giờ, hãy gọi bác sĩ ngay nếu chúng có các triệu chứng sau đây :\\n- Tiêu chảy kéo dài hơn 24 giờ.\\n- Bị tiêu chảy và nôn mửa.\\n- Bị tiêu chảy có phân màu đen hoặc có máu trong phân kèm theo sốt trên 38,5°C.\\n- Rất yếu ớt và từ chối ăn hoặc uống.\\n- Có dấu hiệu mất nước như mắt trũng, da khô, miệng khát, đi tiểu ít hoặc nước tiểu vàng đậm.'}\n"
     ]
    }
   ],
   "source": [
    "llm = load_model(model_file)\n",
    "prompt = creart_prompt(template)\n",
    "db = read_vector_db()\n",
    "\n",
    "llm_chain = create_qa_chain(prompt, llm, db)\n",
    "\n",
    "response = llm_chain.invoke({\"query\": \"Bé bị sốt, phải làm sao?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "healthybot_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
